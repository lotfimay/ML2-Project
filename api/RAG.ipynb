{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the rag bs my dude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY= \"pcsk_5zmUoA_BTvfm3rkJd6H4i7youfn8nQSdZYryD2bgeGpKEMTRhm6wQ6dKFQ7x6bAMkAuci7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample text to encode\n",
    "text = \"This is a test sentence for embeddings.\"\n",
    "\n",
    "# Generate the embedding (a vector representation)\n",
    "embedding = model.encode(text).tolist()  # Convert to list for Pinecone\n",
    "\n",
    "print(\"Vector dimension:\", len(embedding))  # Should be 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ml2 created successfully.\n"
     ]
    }
   ],
   "source": [
    "index_name = \"ml2\"\n",
    "pc = Pinecone(api_key= PINECONE_API_KEY)\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "print(f\"Index {index_name} created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2891 markdown files.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(md_content):\n",
    "    \"\"\"Clean Markdown content by removing HTML tags and irrelevant metadata.\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(md_content, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove extra spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Optional: Remove metadata lines (e.g., \"État: VIGUEUR\")\n",
    "    text = re.sub(r'---.*?---', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def read_markdown_files(root_folder):\n",
    "    md_files = {}\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".md\") and filename.lower() != \"readme.md\":\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        # Clean the file content before storing\n",
    "                        md_files[file_path] = clean_text(file.read())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return md_files\n",
    "\n",
    "# Usage example\n",
    "root_directory = \"./\"\n",
    "markdown_data = read_markdown_files(root_directory)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Found {len(markdown_data)} markdown files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first 5 cleaned markdown files:\n",
      "\n",
      "File: ./LICENCE.md\n",
      "----------------------------------------\n",
      "# Textes juridiques consolidés français sous Git **Avertissement** : Ce projet est en cours de développement. **Il peut contenir des erreurs** ! En cas de doute, nous vous invitons à vous référer au site [Légifrance](https://www.legifrance.gouv.fr/). ## Licence Ce dépôt est constitué d'éléments provenant du projet [Tricoteuses](https://git.tricoteuses.fr/) et de données ouvertes (Open Data) mises à disposition sur le site Légifrance. ### Conditions de réutilisation des données originales du site Légifrance Les données originales sont produites par la [Direction de l'information légale et administrative (Dila)](https://dila.premier-ministre.gouv.fr/). Elles sont réutilisables gratuitement sous [licence ouverte v2.0](https://www.etalab.gouv.fr/licence-ouverte-open-licence/). Les réutilisateurs s'obligent à mentionner : - la paternité des données (DILA) ; - les URL d'accès longues de téléchargement : - https://echanges.dila.gouv.fr/OPENDATA/JORF/ - https://echanges.dila.gouv.fr/OPENDATA/L\n",
      "\n",
      "========================================\n",
      "\n",
      "File: ./livre_ier\\titre\\article_515-10.md\n",
      "----------------------------------------\n",
      " Article 515-10 L'ordonnance de protection est délivrée par le juge, saisi par la personne en danger, si besoin assistée, ou, avec l'accord de celle-ci, par le ministère public. Sa délivrance n'est pas conditionnée à l'existence d'une plainte pénale préalable. Dès la réception de la demande d'ordonnance de protection, le juge convoque, par tous moyens adaptés, pour une audience, la partie demanderesse et la partie défenderesse, assistées, le cas échéant, d'un avocat, ainsi que le ministère public à fin d'avis. Ces auditions peuvent avoir lieu séparément. L'audience se tient en chambre du conseil. A la demande de la partie demanderesse, les auditions se tiennent séparément. Références Articles faisant référence à l'article LOI n° 2019-1480 du 28 décembre 2019 visant à agir contre les violences au sein de la famille - article 2 ENTIEREMENT_MODIF MODIFIE source Références faites par l'article 2019-12-28 MODIFIE cible LOI n° 2019-1480 du 28 décembre 2019 visant à agir contre les violences \n",
      "\n",
      "========================================\n",
      "\n",
      "File: ./livre_ier\\titre\\article_515-11-1.md\n",
      "----------------------------------------\n",
      " Article 515-11-1 I.-Lorsque l'interdiction prévue au 1° de l'article 515-11 a été prononcée, le juge aux affaires familiales peut prononcer une interdiction de se rapprocher de la partie demanderesse à moins d'une certaine distance qu'il fixe et ordonner, après avoir recueilli le consentement des deux parties, le port par chacune d'elles d'un dispositif électronique mobile anti-rapprochement permettant à tout moment de signaler que la partie défenderesse ne respecte pas cette distance. En cas de refus de la partie défenderesse faisant obstacle au prononcé de cette mesure, le juge aux affaires familiales en avise immédiatement le procureur de la République. II.-Ce dispositif fait l'objet d'un traitement de données à caractère personnel, dont les conditions et les modalités de mise en œuvre sont définies par décret en Conseil d'Etat. Références Articles faisant référence à l'article Code civil - article 515-11 AUTONOME MODIFIE, en vigueur du 2022-01-26 au 2024-06-15 CITATION cible Code \n",
      "\n",
      "========================================\n",
      "\n",
      "File: ./livre_ier\\titre\\article_515-11.md\n",
      "----------------------------------------\n",
      " Article 515-11 L'ordonnance de protection est délivrée, par le juge aux affaires familiales, dans un délai maximal de six jours à compter de la fixation de la date de l'audience, s'il estime, au vu des éléments produits devant lui et contradictoirement débattus, qu'il existe des raisons sérieuses de considérer comme vraisemblables, y compris lorsqu'il n'y a pas de cohabitation ou qu'il n'y a jamais eu de cohabitation, la commission des faits de violence allégués et le danger auquel la victime ou un ou plusieurs enfants sont exposés. A l'occasion de sa délivrance, après avoir recueilli les observations des parties sur chacune des mesures suivantes, le juge aux affaires familiales est compétent pour : 1° Interdire à la partie défenderesse de recevoir ou de rencontrer certaines personnes spécialement désignées par le juge aux affaires familiales, ainsi que d'entrer en relation avec elles, de quelque façon que ce soit ; 1° bis Interdire à la partie défenderesse de se rendre dans certains \n",
      "\n",
      "========================================\n",
      "\n",
      "File: ./livre_ier\\titre\\article_515-12.md\n",
      "----------------------------------------\n",
      " Article 515-12 Les mesures mentionnées à l'article 515-11 sont prises pour une durée maximale de douze mois à compter de la notification de l'ordonnance. Elles peuvent être prolongées au-delà si, durant ce délai, une demande en divorce ou en séparation de corps a été déposée ou si le juge aux affaires familiales a été saisi d'une demande relative à l'exercice de l'autorité parentale. Le juge aux affaires familiales peut, à tout moment, à la demande du ministère public ou de l'une ou l'autre des parties, ou après avoir fait procéder à toute mesure d'instruction utile, et après avoir invité chacune d'entre elles à s'exprimer, supprimer ou modifier tout ou partie des mesures énoncées dans l'ordonnance de protection, en décider de nouvelles, accorder à la personne défenderesse une dispense temporaire d'observer certaines des obligations qui lui ont été imposées ou rapporter l'ordonnance de protection. Références Références faites par l'article 2015-03-11 CITATION cible Décret n° 2015-282 \n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Function to print out the cleaned data from markdown files\n",
    "def print_cleaned_markdown_data(cleaned_data, num_files=5):\n",
    "    print(f\"Displaying the first {num_files} cleaned markdown files:\")\n",
    "    \n",
    "    count = 0\n",
    "    for file_path, text in cleaned_data.items():\n",
    "        if count >= num_files:\n",
    "            break\n",
    "        print(f\"\\nFile: {file_path}\\n{'-'*40}\")\n",
    "        print(text[:1000])  # Print the first 1000 characters to avoid too much output\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        count += 1\n",
    "\n",
    "# Usage example\n",
    "print_cleaned_markdown_data(markdown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Markdown Files: 100%|██████████| 2891/2891 [42:53<00:00,  1.12it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a French legal model\n",
    "model_name = \"camembert-base\"  # Suitable model for French legal documents\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Pinecone setup\n",
    "index = pc.Index(\"ml2\")\n",
    "\n",
    "# Step 2: Chunk text with overlap\n",
    "def chunk_text(text, chunk_size=500, overlap_size=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap_size):\n",
    "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Get embeddings using the French model\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    return output.last_hidden_state[:, 0, :].squeeze().numpy()  # Use [CLS] token\n",
    "\n",
    "# Step 4: Insert documents into Pinecone with improved chunking and embeddings\n",
    "def insert_documents():\n",
    "    md_files = markdown_data  # Assuming markdown_data is already loaded\n",
    "    vectors = []\n",
    "\n",
    "    for file_path, content in tqdm(md_files.items(), desc=\"Processing Markdown Files\"):\n",
    "        chunks = chunk_text(content)  # Create chunks with overlap\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = get_embedding(chunk)\n",
    "            vector_id = f\"{file_path}-{i}\"  # Unique ID per chunk\n",
    "            vectors.append({\n",
    "                \"id\": vector_id, \n",
    "                \"values\": embedding.tolist(),\n",
    "                \"metadata\": {\"source\": file_path}\n",
    "            })\n",
    "\n",
    "            # Upsert in batches (Pinecone recommends batching)\n",
    "            if len(vectors) >= 100:\n",
    "                index.upsert(vectors)\n",
    "                vectors = []\n",
    "\n",
    "    if vectors:\n",
    "        index.upsert(vectors)  # Insert remaining vectors\n",
    "\n",
    "# Call the function to insert documents\n",
    "insert_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.954907954\n",
      "Source: ./livre_ier\\titre_v\\chapitre_iv\\article_200.md\n",
      "Content Snippet: No text available\n",
      "--------------------------------------------------\n",
      "Score: 0.951700509\n",
      "Source: ./livre_iii\\titre_xi\\chapitre_iii\\section_3\\article_1963.md\n",
      "Content Snippet: No text available\n",
      "--------------------------------------------------\n",
      "Score: 0.950976193\n",
      "Source: ./livre_iii\\titre_viii\\chapitre_ii\\section_2\\article_1760.md\n",
      "Content Snippet: No text available\n",
      "--------------------------------------------------\n",
      "Score: 0.950892806\n",
      "Source: ./livre_iii\\titre_vi\\chapitre_iv\\section_2\\article_1621.md\n",
      "Content Snippet: No text available\n",
      "--------------------------------------------------\n",
      "Score: 0.950689256\n",
      "Source: ./livre_iii\\titre_xiii\\chapitre_iv\\article_2007.md\n",
      "Content Snippet: No text available\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': './livre_ier\\\\titre_v\\\\chapitre_iv\\\\article_200.md-0',\n",
       "              'metadata': {'source': './livre_ier\\\\titre_v\\\\chapitre_iv\\\\article_200.md'},\n",
       "              'score': 0.954907954,\n",
       "              'values': []},\n",
       "             {'id': './livre_iii\\\\titre_xi\\\\chapitre_iii\\\\section_3\\\\article_1963.md-0',\n",
       "              'metadata': {'source': './livre_iii\\\\titre_xi\\\\chapitre_iii\\\\section_3\\\\article_1963.md'},\n",
       "              'score': 0.951700509,\n",
       "              'values': []},\n",
       "             {'id': './livre_iii\\\\titre_viii\\\\chapitre_ii\\\\section_2\\\\article_1760.md-0',\n",
       "              'metadata': {'source': './livre_iii\\\\titre_viii\\\\chapitre_ii\\\\section_2\\\\article_1760.md'},\n",
       "              'score': 0.950976193,\n",
       "              'values': []},\n",
       "             {'id': './livre_iii\\\\titre_vi\\\\chapitre_iv\\\\section_2\\\\article_1621.md-0',\n",
       "              'metadata': {'source': './livre_iii\\\\titre_vi\\\\chapitre_iv\\\\section_2\\\\article_1621.md'},\n",
       "              'score': 0.950892806,\n",
       "              'values': []},\n",
       "             {'id': './livre_iii\\\\titre_xiii\\\\chapitre_iv\\\\article_2007.md-0',\n",
       "              'metadata': {'source': './livre_iii\\\\titre_xiii\\\\chapitre_iv\\\\article_2007.md'},\n",
       "              'score': 0.950689256,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_pinecone(query, top_k=5):\n",
    "    query_embedding = get_embedding(query)  # Convert question to vector\n",
    "    results = index.query(vector=query_embedding.tolist(), top_k=top_k, include_metadata=True)\n",
    "\n",
    "    for match in results[\"matches\"]:\n",
    "        print(f\"Score: {match['score']}\")\n",
    "        print(f\"Source: {match['metadata']['source']}\")\n",
    "        print(\"Content Snippet:\", match[\"metadata\"].get(\"text\", \"No text available\"))\n",
    "        print(\"-\" * 50)\n",
    "    return results\n",
    "\n",
    "# Run the test query\n",
    "query_pinecone(\"\"\"L'ordonnance de protection est délivrée par le juge, saisi par la personne en\n",
    "danger, si besoin assistée, ou, avec l'accord de celle-ci, par le ministère\n",
    "public.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m initial_results \u001b[38;5;241m=\u001b[39m query_pinecone(query)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 2. Rerank the results using MS MARCO\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m reranked_results \u001b[38;5;241m=\u001b[39m \u001b[43mrerank_with_msmarco\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Print reranked results\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReranked results for the query \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[61], line 16\u001b[0m, in \u001b[0;36mrerank_with_msmarco\u001b[1;34m(query, retrieved_results)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m rerank_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m---> 16\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# The score is in logits\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     rerank_scores\u001b[38;5;241m.\u001b[39mappend((score, result))  \u001b[38;5;66;03m# Store the score along with the result\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Sort the results by score (higher score means more relevant)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# Load the MS MARCO reranking model\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Function to rerank results using MS MARCO\n",
    "def rerank_with_msmarco(query, retrieved_results):\n",
    "    rerank_scores = []\n",
    "    \n",
    "    for result in retrieved_results:\n",
    "        doc_text = result['metadata']['source']  # Assume document text or metadata is in source\n",
    "        inputs = rerank_tokenizer([query, doc_text], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get the prediction score from the model\n",
    "        with torch.no_grad():\n",
    "            outputs = rerank_model(**inputs)\n",
    "            score = outputs.logits.squeeze().item()  # The score is in logits\n",
    "        \n",
    "        rerank_scores.append((score, result))  # Store the score along with the result\n",
    "\n",
    "    # Sort the results by score (higher score means more relevant)\n",
    "    reranked_results = sorted(rerank_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [result for score, result in reranked_results]\n",
    "\n",
    "# Function to query Pinecone and return results\n",
    "def query_pinecone(query, top_k=5):\n",
    "    query_embedding = get_embedding(query)  # Convert query to vector\n",
    "    results = index.query(vector=query_embedding.tolist(), top_k=top_k, include_metadata=True)\n",
    "\n",
    "    return results[\"matches\"]\n",
    "\n",
    "# Example of querying Pinecone and reranking results\n",
    "query = \"Quel est le statut des récoltes pendantes ?\"\n",
    "\n",
    "# 1. Query Pinecone to get initial results\n",
    "initial_results = query_pinecone(query)\n",
    "\n",
    "# 2. Rerank the results using MS MARCO\n",
    "reranked_results = rerank_with_msmarco(query, initial_results)\n",
    "\n",
    "# Print reranked results\n",
    "print(f\"Reranked results for the query '{query}':\")\n",
    "for result in reranked_results:\n",
    "    print(f\"Source: {result['metadata']['source']}, Score: {result['score']}\")\n",
    "    print(\"Content Snippet:\", result[\"metadata\"].get(\"text\", \"No text available\"))\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
